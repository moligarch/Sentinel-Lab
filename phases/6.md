# Phase 6 (Milestone M20): LLM Services, Evaluation & Hardening

This final phase integrates Large Language Models into the system and focuses on production readiness. The logical order is to learn the necessary LLM concepts, build the core serving infrastructure, create the evaluation harness to ensure quality, integrate the AI capabilities into the existing system, and finally, perform the end-to-end evaluation and operational hardening.

1.  `[Learn] Transformers & LLM Serving` - Understand the core technology and its operational challenges.
2.  `[Learn] LLM Evaluation Frameworks (Ragas)` - Learn how to measure the quality of LLM output.
3.  `[Build] llm-gateway: Deploy vLLM Server` - Set up the core engine for serving models.
4.  `[Build] llm-gateway: API Gateway Service` - Create the critical proxy for auth, rate limiting, and logging.
5.  `[Build] eval-harness: Create Golden Datasets` - Develop the ground truth for evaluating model performance.
6.  `[Build] eval-harness: Ragas Evaluation Pipeline` - Automate the process of scoring LLM responses.
7.  `[Build] policy-orchestrator: Integrate LLM Tool-Calling` - Connect the AI capabilities to the live system.
8.  `[Eval] Compare LLM Models and Prompts` - The main evaluation deliverable to choose the best configuration.
9.  `[Eval] System Reliability via Chaos Experiment` - Test the resilience of the entire system.
10. `[Doc] Production Readiness Playbooks` - Create the final operational documentation.

---
---

### 1. [Learn] Transformers & LLM Serving

### Resource
*Natural Language Processing with Transformers* (Hugging Face / O'Reilly) and the vLLM documentation.

### Phase
P6

### Module (if applicable)
llm-gateway

### Goals / Why
To understand the fundamentals of the Transformer architecture that powers modern LLMs. More importantly, to learn the practical challenges of serving these models, such as managing VRAM, optimizing for throughput with techniques like paged attention (which vLLM uses), and handling batched inference.

### Notes / Summary
-   Summarize the role of the self-attention mechanism in Transformers.
-   Diagram the architecture of an LLM serving stack (e.g., Gateway -> Inference Server -> GPU).
-   Explain why traditional web server scaling (more replicas) doesn't work well for LLMs and what alternative strategies are used.

### Definition of Done
- [ ] A summary of LLM serving challenges is committed to `/docs/learning/llm-serving-fundamentals.md`.
- [ ] A small Python script using the `transformers` library to run inference with a pre-trained model is created.
- [ ] Linked to milestone & project.

---

### 2. [Learn] LLM Evaluation Frameworks (Ragas)

### Resource
The official Ragas documentation and related blog posts on RAG evaluation.

### Phase
P6

### Module (if applicable)
eval-harness

### Goals / Why
To understand that evaluating generative AI is different from traditional software testing. The goal is to learn the methodologies for evaluating LLM-powered systems, especially for RAG (Retrieval-Augmented Generation) use cases. This involves understanding metrics like **faithfulness** (did the model hallucinate?), **context recall** (did the context contain the right info?), and **answer relevancy**.

### Notes / Summary
-   Define each of the core Ragas metrics in your own words.
-   Create a small table comparing "component-wise" evaluation (testing each part of the RAG pipeline) vs. "end-to-end" evaluation.
-   Write a small Python script that uses Ragas to evaluate a single, hardcoded example (question, context, answer, ground truth).

### Definition of Done
- [ ] A summary of RAG evaluation metrics is committed to `/docs/learning/llm-evaluation-metrics.md`.
- [ ] The runnable Ragas example script is committed to a learning directory.
- [ ] Linked to milestone & project.

---

### 3. [Build] llm-gateway: Deploy vLLM Server

### Module
llm-gateway

### Phase
P6

### Design sketch / interfaces / contracts
-   Use the official vLLM Docker image to add a new service to the main `docker-compose.yml` file.
-   The service will be configured to download and serve a small, open-source model (e.g., a quantized version of Mistral-7B or a similar model).
-   The vLLM container should expose its OpenAI-compatible API endpoint to the internal Docker network.
-   Configure the container to leverage a GPU if available in the local development environment.

### Test Plan
-   After `docker compose up`, send a `curl` request to the vLLM container's `/v1/completions` endpoint from another container and verify that it returns a valid LLM response.

### Definition of Done
- [ ] The `vllm` service is defined in `docker-compose.yml`.
- [ ] The `README.md` is updated with instructions on how to select a model and run the service.
- [ ] Linked to milestone & project.

---

### 4. [Build] llm-gateway: API Gateway Service

### Module
llm-gateway

### Phase
P6

### Design sketch / interfaces / contracts
-   Create a new proxy service (e.g., in Python or Go) that acts as the single entry point for all internal LLM calls.
-   **Features**:
    -   It will receive requests on its own API endpoint and forward them to the backend vLLM server.
    -   **Authentication**: It will require an API key or internal service token for all requests.
    -   **Rate Limiting/Quotas**: Implement per-client rate limiting.
    -   **Logging/Tracing**: Log every request and its latency, and propagate OpenTelemetry trace IDs.
    -   **Tool/Function-Calling Adapter**: Expose a simplified API for common tasks (e.g., `/v1/summarize`) that translates the request into a detailed prompt for the LLM.

### Test Plan
-   Unit tests for authentication and rate-limiting logic.
-   Integration test: a request to the gateway with a valid token should succeed, while a request with an invalid token should fail with a `401/403` error.

### Definition of Done
- [ ] The `llm-gateway` service code is created.
- [ ] The service is containerized and added to `docker-compose.yml`.
- [ ] The README documents the API endpoints and authentication mechanism.
- [ ] Linked to milestone & project.

---

### 5. [Build] eval-harness: Create Golden Datasets

### Module
eval-harness

### Phase
P6

### Design sketch / interfaces / contracts
-   Create a directory `/eval-harness/datasets/`.
-   Develop a "golden dataset" for a key task, such as summarizing security events.
-   The dataset will be in a structured format like JSONL.
-   **Each record will contain**:
    -   `event_id`: A unique identifier.
    -   `context`: The raw security event data (e.g., a JSON blob).
    -   `question`: The task to be performed (e.g., "Summarize this process creation event in one sentence.").
    -   `ground_truth`: A human-written, ideal answer to the question.

### Test Plan
-   Write a simple script to parse and validate the format of the dataset file to ensure all required keys are present.

### Definition of Done
- [ ] A golden dataset with at least 20-30 high-quality examples is created and committed.
- [ ] The schema for the dataset is documented in the `eval-harness` README.
- [ ] Linked to milestone & project.

---

### 6. [Build] eval-harness: Ragas Evaluation Pipeline

### Module
eval-harness

### Phase
P6

### Design sketch / interfaces / contracts
-   Create a Python script within the `eval-harness` module that automates LLM evaluation.
-   **Pipeline Logic**:
    1.  Load the golden dataset.
    2.  For each item in the dataset, make an API call to the `llm-gateway` to get the model's generated answer.
    3.  Collect the `question`, `context`, `generated_answer`, and `ground_truth` for each item.
    4.  Use the `ragas` library to evaluate the collected data against metrics like `faithfulness`, `answer_relevancy`, and `context_recall`.
    5.  Print a final report with the average scores for each metric.

### Test Plan
-   Run the pipeline against a small, 3-5 item subset of the dataset and verify that it completes and produces a score report.
-   Integrate the pipeline into CI to run on a nightly schedule or on-change.

### Definition of Done
- [ ] The evaluation pipeline script is created.
- [ ] The script can be run with a single command (e.g., `make eval`).
- [ ] The CI job is created to run the evaluation pipeline.
- [ ] Linked to milestone & project.

---

### 7. [Build] policy-orchestrator: Integrate LLM Tool-Calling

### Module
policy-orchestrator

### Phase
P6

### Design sketch / interfaces / contracts
-   Modify the API exposed from Rust to the Rhai `rule-engine`.
-   Create a new, simplified function accessible from Rhai, such as `llm.summarize(event_data)`.
-   When this function is called from a rule script, the `policy-orchestrator` (the Rust host) will:
    1.  Construct a prompt based on the event data.
    2.  Make an authenticated gRPC or HTTP call to the `llm-gateway` service.
    3.  Receive the response and return the summarized text to the Rhai script.
-   Update a sample rule to use this new function.

### Test Plan
-   Write an integration test that triggers a rule that calls the `llm.summarize` function.
-   Mock the `llm-gateway` and assert that the `policy-orchestrator` makes the correct outbound API call.

### Definition of Done
- [ ] The `llm` module is available to Rhai scripts.
- [ ] The orchestrator can successfully call the `llm-gateway`.
- [ ] A sample rule is updated to demonstrate the new capability.
- [ ] Linked to milestone & project.

---

### 8. [Eval] Compare LLM Models and Prompts

### Area
RAG/LLM

### Metrics & targets
-   Identify the configuration with the highest average `faithfulness` and `answer_relevancy` scores from Ragas.
-   Measure the p99 latency for each configuration to understand performance trade-offs.

### Plan
1.  Configure the `eval-harness` pipeline to be parameterizable (e.g., it can be pointed at different models served by vLLM or use different prompt templates).
2.  Run the full evaluation pipeline against the golden dataset for at least three different configurations:
    -   Configuration A: Model A with Prompt Template 1.
    -   Configuration B: Model B with Prompt Template 1.
    -   Configuration C: Model A with Prompt Template 2.
3.  Collect the Ragas scores and latency metrics for each run.

### Definition of Done
- [ ] An evaluation report is created in `/docs/eval/M20-llm-comparison.md`.
- [ ] The report includes a table comparing the scores and performance of each configuration.
- [ ] A final recommendation for the best model/prompt configuration is made based on the data.

---

### 9. [Eval] System Reliability via Chaos Experiment

### Area
SLO

### Metrics & targets
-   The system should recover automatically with no manual intervention.
-   The relevant SLO alerts (e.g., for `llm-gateway` latency or error rate) should fire correctly.
-   Data loss should be zero for the core pipeline.

### Plan
1.  Start a light, constant stream of events through the entire system.
2.  While the system is running, conduct a chaos experiment using a tool like `docker kill` or by manipulating network rules with `tc`.
3.  **Scenario**: Kill the `llm-gateway` container.
4.  **Observe**:
    -   Does the `policy-orchestrator` handle the failure gracefully (e.g., with timeouts, circuit breakers)?
    -   Does Docker Swarm or Kubernetes restart the container?
    -   Do alerts fire for the gateway being down?
    -   Does the system recover and resume processing once the gateway is back up?

### Definition of Done
- [ ] The chaos experiment procedure and its outcome are documented in a runbook.
- [ ] One improvement is made to the system's resilience based on the findings.
- [ ] Linked to milestone & project.

---

### 10. [Doc] Production Readiness Playbooks

### Phase
P6

### Summary
Create the final set of operational documents required to run the entire system in a production-like environment. This represents the culmination of the project's SRE and MLOps efforts.

### Where to put it
`/docs/operations/`

### Checklist / Extras
- [ ] Create an **Incident Response Playbook** that details steps to take for common alerts.
- [ ] Create a **Postmortem Template** to be used after any significant incident.
- [ ] Create a **Runbook** for common operational tasks, such as rotating API keys for the `llm-gateway` or backfilling features with the batch job.