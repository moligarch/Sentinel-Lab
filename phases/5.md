# Phase 5 (Milestone M17): Data Platform & AIOps/MLOps Foundations

This phase builds the data and MLOps backbone of the system. The logical order is to start with foundational learning, make key architectural decisions about the data infrastructure, then build the platform components, and finally enhance the system's operational maturity with formal SLOs.

1.  `[Learn] Designing Data-Intensive Applications` - The theoretical foundation for the entire phase.
2.  `[Learn] Messaging Systems Deep Dive (Kafka & NATS)` - Prerequisite for the message bus decision.
3.  `[Learn] MLOps & Data Engineering Fundamentals` - Understand the "why" behind the MLOps tooling.
4.  `[ADR] Production Message Bus Selection (NATS vs. Kafka)` - A critical architectural decision.
5.  `[ADR] Data Lake Storage and Schema Strategy` - Define how data will be stored at rest.
6.  `[Build] mlops/data: Core Tooling in Docker Compose` - Set up the core infrastructure.
7.  `[Build] mlops/data: Data Validation with Great Expectations` - Implement data quality gates.
8.  `[Build] mlops/data: Feast Feature Store Definitions` - Define the features for ML models.
9.  `[Build] mlops/data: Batch & Streaming Feature Engineering Jobs` - The core deliverable: materialize the features.
10. `[Build] observability: Define and Dashboard Service SLOs` - Formalize and monitor service reliability.
11. `[Doc] Create Reproducible `make demo` Target` - Document and automate an end-to-end flow.

---
---

### 1. [Learn] Designing Data-Intensive Applications

### Resource
*Designing Data-Intensive Applications* (by Martin Kleppmann).

### Phase
P5

### Module (if applicable)
mlops/data

### Goals / Why
To understand the fundamental principles of building reliable, scalable, and maintainable data systems. This book provides the essential theoretical background for making informed decisions about databases, caches, message brokers, and data processing frameworks.

### Notes / Summary
-   Summarize the trade-offs between different data models (Relational vs. Document vs. Graph).
-   Explain the concepts of replication, partitioning, and transactions.
-   Diagram the difference between batch processing and stream processing.

### Definition of Done
- [ ] Chapter summaries for Parts I and II are committed to `/docs/learning/ddia-summary.md`.
- [ ] A small prototype demonstrating a concept (e.g., a batch vs. stream processing script) is created.
- [ ] Linked to milestone & project.

---

### 2. [Learn] Messaging Systems Deep Dive (Kafka & NATS)

### Resource
*Kafka: The Definitive Guide (2e)* and the official NATS JetStream documentation.

### Phase
P5

### Module (if applicable)
event-bus

### Goals / Why
To move beyond a superficial understanding of the message bus and deeply understand the architectural differences and trade-offs between Kafka and NATS JetStream. This knowledge is critical for making the final "production" message bus decision.

### Notes / Summary
-   Compare Kafka's partitioned log architecture with NATS's subject-based filtering.
-   Explain the delivery semantics (at-least-once, exactly-once) in both systems.
-   Detail the operational differences regarding persistence, replication, and consumer groups.

### Definition of Done
- [ ] A comparison table is created in `/docs/learning/kafka-vs-nats.md`.
- [ ] A small test is run to demonstrate the persistence guarantees of each system.
- [ ] Linked to milestone & project.

---

### 3. [Learn] MLOps & Data Engineering Fundamentals

### Resource
*Fundamentals of Data Engineering* (Reis & Housley) and *Designing Machine Learning Systems* (Chip Huyen).

### Phase
P5

### Module (if applicable)
mlops/data

### Goals / Why
To understand the end-to-end lifecycle of data and models in a production environment. This provides the context for why tools like feature stores (Feast), data validation (Great Expectations), and model registries (MLflow) are necessary.

### Notes / Summary
-   Diagram the typical data engineering lifecycle (e.g., Bronze/Silver/Gold tables).
-   Summarize the core components of an MLOps platform (e.g., experiment tracking, feature store, model serving, monitoring).
-   Explain the concept of online/offline parity for feature engineering.

### Definition of Done
- [ ] A summary of the MLOps/Data Engineering lifecycle is committed to `/docs/learning/mlops-data-eng-fundamentals.md`.
- [ ] Linked to milestone & project.

---

### 4. [ADR] Production Message Bus Selection (NATS vs. Kafka)

### Context
The project currently uses a simple message bus for local development. For a system that will support data retention for model training and reprocessing, a formal decision on a production-grade, persistent message bus is required.

### Options
1.  **Kafka**: Industry standard for durable, partitioned logs. Offers strong ordering guarantees and a massive ecosystem, but can be operationally complex.
2.  **NATS JetStream**: A modern, simpler alternative that adds persistence, streaming, and exactly-once semantics to NATS. Easier to operate but has a smaller ecosystem.

### Decision
Select one platform (e.g., "We will adopt Kafka as the primary event bus"). The rationale should focus on long-term data retention needs, replay capabilities, and integration with stream processing frameworks.

### Consequences
This decision will require migrating existing services to new client libraries and will dictate the data ingestion patterns for the rest of the project.

### File path"
`/docs/decisions/ADR-000X-production-message-bus.md`

### Follow-up Checklist
- [ ] Created doc file and linked this issue.
- [ ] Created follow-up tickets to migrate existing services to the new client library.

---

### 5. [ADR] Data Lake Storage and Schema Strategy

### Context
Raw events from the event bus need to be durably stored in a "data lake" for historical analysis, model training, and rule backtesting. We need a strategy for the storage format, partitioning, and schema management.

### Options
-   **Storage Format**: Parquet, Avro, ORC.
-   **Schema Management**: Confluent Schema Registry, ad-hoc JSON Schemas.

### Decision
-   **Format**: Adopt Apache Parquet for its columnar storage efficiency and broad tool compatibility.
-   **Partitioning**: Partition data in storage by date (e.g., `/raw/YYYY/MM/DD/`).
-   **Schema**: Use Avro for defining event schemas and integrate with a schema registry to enforce schema evolution rules.

### Consequences
This provides a scalable and queryable foundation for all future batch processing and ML training jobs. All data producers must adhere to the schema registry.

### File path"
`/docs/decisions/ADR-000X-data-lake-strategy.md`

### Follow-up Checklist
- [ ] Created doc file and linked this issue.
- [ ] Set up a schema registry service in the docker-compose stack.

---

### 6. [Build] mlops/data: Core Tooling in Docker Compose

### Module
mlops/data

### Phase
P5

### Design sketch / interfaces / contracts
-   Extend the main `docker-compose.yml` to include the core MLOps and Data stack.
-   **MLflow**: Add an MLflow tracking server container with a PostgreSQL backend for storing experiments and a model registry.
-   **Feast**: Add Feast services for feature serving, configured to use a Redis container for the online store and a local file source for the offline store.
-   **Great Expectations**: Set up a directory structure for a GX project.

### Test Plan
-   After `docker compose up`, verify that the MLflow and Feast UIs are accessible.
-   Run the `great_expectations init` command to confirm the project is set up correctly.

### Definition of Done
- [ ] All new services are defined and networked in the `docker-compose.yml` file.
- [ ] A `README.md` in the `mlops/data` module explains how to access each service.
- [ ] Linked to milestone & project.

---

### 7. [Build] mlops/data: Data Validation with Great Expectations

### Module
mlops/data

### Phase
P5

### Design sketch / interfaces / contracts
-   Initialize a Great Expectations (GX) project within the `mlops/data` module.
-   Create a "Checkpoint" that can be run against a batch of raw event data.
-   Develop an "Expectation Suite" for the raw sensor event schema.
-   **Expectations**:
    -   `expect_column_to_not_be_null`: for `timestamp`, `sensor_id`.
    -   `expect_column_values_to_match_regex`: for IP addresses.
    -   `expect_column_values_to_be_in_set`: for `event_type`.

### Test Plan
-   Create a sample batch of data with known "good" and "bad" records.
-   Run the GX Checkpoint against the data and assert that it correctly identifies the validation failures.
-   Integrate the GX Checkpoint into the CI pipeline to run on every commit.

### Definition of Done
- [ ] The GX project is created and checked into the repository.
- [ ] The "raw_events" Expectation Suite is defined.
- [ ] The GX Checkpoint is added as a step in the CI pipeline, and it must pass.
- [ ] Linked to milestone & project.

---

### 8. [Build] mlops/data: Feast Feature Store Definitions

### Module
mlops/data

### Phase
P5

### Design sketch / interfaces / contracts
-   Create a Feast feature repository inside the `mlops/data` module.
-   Define data sources that point to the raw event data (e.g., Parquet files in the data lake).
-   Define `Entities` (e.g., `user`, `hostname`).
-   Define `FeatureViews` that describe how to compute features from the source data.
-   **Feature Examples**:
    -   `user_process_spawn_count_24h`: A user's total process creation count over 24 hours.
    -   `hostname_unique_dns_queries_1h`: Number of unique domains queried by a host in 1 hour.

### Test Plan
-   Run `feast plan` and `feast apply` to register the definitions with the feature store.
-   Write a small Python script that attempts to fetch feature values for a test entity.

### Definition of Done
- [ ] The Feast `feature_repo.yaml` and feature definition files (`.py`) are created.
- [ ] The feature repository is successfully applied to the local Feast infrastructure.
- [ ] Linked to milestone & project.

---

### 9. [Build] mlops/data: Batch & Streaming Feature Engineering Jobs

### Module
mlops/data

### Phase
P5

### Design sketch / interfaces / contracts
-   Create two Python scripts in the `mlops/data` module to compute and materialize features.
-   **1. Batch Job**:
    -   A script that reads historical raw data (e.g., from Parquet files).
    -   Uses a library like Pandas or Polars to perform windowed aggregations to compute the features defined in Feast.
    -   Calls `feast materialize-incremental` to populate the Feast online store.
-   **2. Streaming Job**:
    -   A long-running service that consumes events from the production message bus (Kafka/NATS).
    -   Performs similar aggregations in-memory for a sliding time window.
    -   Pushes the computed features directly to the Feast online store (Redis).

### Test Plan
-   **Batch**: Run the job against a sample dataset and verify that the features appear in the Feast online store.
-   **Streaming**: Publish mock events to the bus and query the Feast online store to verify that the features are updated in near real-time.

### Definition of Done
- [ ] The batch and streaming job scripts are created.
- [ ] Both jobs are containerized and can be run via `docker compose`.
- [ ] The CI pipeline includes a step to run the batch job.
- [ ] Linked to milestone & project.

---

### 10. [Build] observability: Define and Dashboard Service SLOs

### Module
observability

### Phase
P5

### Design sketch / interfaces / contracts
-   Formalize Service Level Objectives (SLOs) for critical user journeys.
-   **Example SLOs**:
    -   **Ingest Availability**: 99.9% of `/ingest` requests should succeed over a 28-day window.
    -   **Enrichment Latency**: 99% of events should be processed by `enrichment-services` in under 100ms.
-   Write PromQL queries to calculate the Service Level Indicators (SLIs) for these SLOs.
-   Create a new Grafana dashboard dedicated to visualizing the SLOs, their current status, and the remaining error budget for the period.

### Test Plan
-   Generate both successful and failed traffic to the services.
-   Verify that the SLI metrics and error budget calculations on the dashboard are correct.

### Definition of Done
- [ ] A file `/observability/slos.yaml` formally defines the SLOs.
- [ ] The Grafana SLO dashboard is created and saved as JSON in the repository.
- [ ] An alert is configured to fire if the error budget is burning down too quickly.
- [ ] Linked to milestone & project.

---

### 11. [Doc] Create Reproducible `make demo` Target

### Phase
P5

### Summary
Create a `make demo` target in the root Makefile that automates a full end-to-end demonstration of the system. This serves as a powerful tool for onboarding, testing, and ensuring the project is always in a runnable state.

### Where to put it
The logic will be in the root `Makefile`, and it should be documented in the main `README.md`.

### Checklist / Extras
- [ ] The script successfully runs `docker compose up -d`.
- [ ] The script runs a mock sensor to publish a sample event.
- [ ] The script shows logs from the `enrichment-services` and `rule-engine` processing the event.
- [ ] The script runs a Python client to query Feast and show the materialized feature.
- [ ] The script runs `docker compose down` for cleanup.